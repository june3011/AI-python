import sklearn

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
# 분류 -> 지도학습 -> 결정 알고리즘

# 데이터 로드
iris = load_iris()
iris
# {'DESCR': '.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n     Mathematical Statistics" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...',
#  'data': array([[5.1, 3.5, 1.4, 0.2],
#         [4.9, 3. , 1.4, 0.2],
#         [4.7, 3.2, 1.3, 0.2],
#         [4.6, 3.1, 1.5, 0.2],
#         [5. , 3.6, 1.4, 0.2],
#         [5.4, 3.9, 1.7, 0.4],
#         [4.6, 3.4, 1.4, 0.3],
#         [5. , 3.4, 1.5, 0.2],
#         [4.4, 2.9, 1.4, 0.2],
#         [4.9, 3.1, 1.5, 0.1],
#         [5.4, 3.7, 1.5, 0.2],
#         [4.8, 3.4, 1.6, 0.2],
#         [4.8, 3. , 1.4, 0.1],
#         [4.3, 3. , 1.1, 0.1],
#         [5.8, 4. , 1.2, 0.2],
#         [5.7, 4.4, 1.5, 0.4],
#         [5.4, 3.9, 1.3, 0.4],
#         [5.1, 3.5, 1.4, 0.3],
#         [5.7, 3.8, 1.7, 0.3],
#         [5.1, 3.8, 1.5, 0.3],
#         [5.4, 3.4, 1.7, 0.2],
#         [5.1, 3.7, 1.5, 0.4],
#         [4.6, 3.6, 1. , 0.2],
#         [5.1, 3.3, 1.7, 0.5],
#         [4.8, 3.4, 1.9, 0.2],
#         [5. , 3. , 1.6, 0.2],
#         [5. , 3.4, 1.6, 0.4],
#         [5.2, 3.5, 1.5, 0.2],
#         [5.2, 3.4, 1.4, 0.2],
#         [4.7, 3.2, 1.6, 0.2],
#         [4.8, 3.1, 1.6, 0.2],
#         [5.4, 3.4, 1.5, 0.4],
#         [5.2, 4.1, 1.5, 0.1],
#         [5.5, 4.2, 1.4, 0.2],
#         [4.9, 3.1, 1.5, 0.2],
#         [5. , 3.2, 1.2, 0.2],
#         [5.5, 3.5, 1.3, 0.2],
#         [4.9, 3.6, 1.4, 0.1],
#         [4.4, 3. , 1.3, 0.2],
#         [5.1, 3.4, 1.5, 0.2],
#         [5. , 3.5, 1.3, 0.3],
#         [4.5, 2.3, 1.3, 0.3],
#         [4.4, 3.2, 1.3, 0.2],
#         [5. , 3.5, 1.6, 0.6],
#         [5.1, 3.8, 1.9, 0.4],
#         [4.8, 3. , 1.4, 0.3],
#         [5.1, 3.8, 1.6, 0.2],
#         [4.6, 3.2, 1.4, 0.2],
#         [5.3, 3.7, 1.5, 0.2],
#         [5. , 3.3, 1.4, 0.2],
#         [7. , 3.2, 4.7, 1.4],
#         [6.4, 3.2, 4.5, 1.5],
#         [6.9, 3.1, 4.9, 1.5],
#         [5.5, 2.3, 4. , 1.3],
#         [6.5, 2.8, 4.6, 1.5],
#         [5.7, 2.8, 4.5, 1.3],
#         [6.3, 3.3, 4.7, 1.6],
#         [4.9, 2.4, 3.3, 1. ],
#         [6.6, 2.9, 4.6, 1.3],
#         [5.2, 2.7, 3.9, 1.4],
#         [5. , 2. , 3.5, 1. ],
#         [5.9, 3. , 4.2, 1.5],
#         [6. , 2.2, 4. , 1. ],
#         [6.1, 2.9, 4.7, 1.4],
#         [5.6, 2.9, 3.6, 1.3],
#         [6.7, 3.1, 4.4, 1.4],
#         [5.6, 3. , 4.5, 1.5],
#         [5.8, 2.7, 4.1, 1. ],
#         [6.2, 2.2, 4.5, 1.5],
#         [5.6, 2.5, 3.9, 1.1],
#         [5.9, 3.2, 4.8, 1.8],
#         [6.1, 2.8, 4. , 1.3],
#         [6.3, 2.5, 4.9, 1.5],
#         [6.1, 2.8, 4.7, 1.2],
#         [6.4, 2.9, 4.3, 1.3],
#         [6.6, 3. , 4.4, 1.4],
#         [6.8, 2.8, 4.8, 1.4],
#         [6.7, 3. , 5. , 1.7],
#         [6. , 2.9, 4.5, 1.5],
#         [5.7, 2.6, 3.5, 1. ],
#         [5.5, 2.4, 3.8, 1.1],
#         [5.5, 2.4, 3.7, 1. ],
#         [5.8, 2.7, 3.9, 1.2],
#         [6. , 2.7, 5.1, 1.6],
#         [5.4, 3. , 4.5, 1.5],
#         [6. , 3.4, 4.5, 1.6],
#         [6.7, 3.1, 4.7, 1.5],
#         [6.3, 2.3, 4.4, 1.3],
#         [5.6, 3. , 4.1, 1.3],
#         [5.5, 2.5, 4. , 1.3],
#         [5.5, 2.6, 4.4, 1.2],
#         [6.1, 3. , 4.6, 1.4],
#         [5.8, 2.6, 4. , 1.2],
#         [5. , 2.3, 3.3, 1. ],
#         [5.6, 2.7, 4.2, 1.3],
#         [5.7, 3. , 4.2, 1.2],
#         [5.7, 2.9, 4.2, 1.3],
#         [6.2, 2.9, 4.3, 1.3],
#         [5.1, 2.5, 3. , 1.1],
#         [5.7, 2.8, 4.1, 1.3],
#         [6.3, 3.3, 6. , 2.5],
#         [5.8, 2.7, 5.1, 1.9],
#         [7.1, 3. , 5.9, 2.1],
#         [6.3, 2.9, 5.6, 1.8],
#         [6.5, 3. , 5.8, 2.2],
#         [7.6, 3. , 6.6, 2.1],
#         [4.9, 2.5, 4.5, 1.7],
#         [7.3, 2.9, 6.3, 1.8],
#         [6.7, 2.5, 5.8, 1.8],
#         [7.2, 3.6, 6.1, 2.5],
#         [6.5, 3.2, 5.1, 2. ],
#         [6.4, 2.7, 5.3, 1.9],
#         [6.8, 3. , 5.5, 2.1],
#         [5.7, 2.5, 5. , 2. ],
#         [5.8, 2.8, 5.1, 2.4],
#         [6.4, 3.2, 5.3, 2.3],
#         [6.5, 3. , 5.5, 1.8],
#         [7.7, 3.8, 6.7, 2.2],
#         [7.7, 2.6, 6.9, 2.3],
#         [6. , 2.2, 5. , 1.5],
#         [6.9, 3.2, 5.7, 2.3],
#         [5.6, 2.8, 4.9, 2. ],
#         [7.7, 2.8, 6.7, 2. ],
#         [6.3, 2.7, 4.9, 1.8],
#         [6.7, 3.3, 5.7, 2.1],
#         [7.2, 3.2, 6. , 1.8],
#         [6.2, 2.8, 4.8, 1.8],
#         [6.1, 3. , 4.9, 1.8],
#         [6.4, 2.8, 5.6, 2.1],
#         [7.2, 3. , 5.8, 1.6],
#         [7.4, 2.8, 6.1, 1.9],
#         [7.9, 3.8, 6.4, 2. ],
#         [6.4, 2.8, 5.6, 2.2],
#         [6.3, 2.8, 5.1, 1.5],
#         [6.1, 2.6, 5.6, 1.4],
#         [7.7, 3. , 6.1, 2.3],
#         [6.3, 3.4, 5.6, 2.4],
#         [6.4, 3.1, 5.5, 1.8],
#         [6. , 3. , 4.8, 1.8],
#         [6.9, 3.1, 5.4, 2.1],
#         [6.7, 3.1, 5.6, 2.4],
#         [6.9, 3.1, 5.1, 2.3],
#         [5.8, 2.7, 5.1, 1.9],
#         [6.8, 3.2, 5.9, 2.3],
#         [6.7, 3.3, 5.7, 2.5],
#         [6.7, 3. , 5.2, 2.3],
#         [6.3, 2.5, 5. , 1.9],
#         [6.5, 3. , 5.2, 2. ],
#         [6.2, 3.4, 5.4, 2.3],
#         [5.9, 3. , 5.1, 1.8]]),
#  'data_module': 'sklearn.datasets.data',
#  'feature_names': ['sepal length (cm)',
#   'sepal width (cm)',
#   'petal length (cm)',
#   'petal width (cm)'],
#  'filename': 'iris.csv',
#  'frame': None,
#  'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),
#  'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}




# iris.data 는 피처(feature)로 만으로 된 데이터 (ndarray 객체)
iris_data = iris.data
iris_data
# array([[5.1, 3.5, 1.4, 0.2],
#        [4.9, 3. , 1.4, 0.2],
#        [4.7, 3.2, 1.3, 0.2],
#        [4.6, 3.1, 1.5, 0.2],
#        [5. , 3.6, 1.4, 0.2],
#        [5.4, 3.9, 1.7, 0.4],
#        [4.6, 3.4, 1.4, 0.3],
#        [5. , 3.4, 1.5, 0.2],
#        [4.4, 2.9, 1.4, 0.2],
#        [4.9, 3.1, 1.5, 0.1],
#        [5.4, 3.7, 1.5, 0.2],
#        [4.8, 3.4, 1.6, 0.2],
#        [4.8, 3. , 1.4, 0.1],
#        [4.3, 3. , 1.1, 0.1],
#        [5.8, 4. , 1.2, 0.2],
#        [5.7, 4.4, 1.5, 0.4],
#        [5.4, 3.9, 1.3, 0.4],
#        [5.1, 3.5, 1.4, 0.3],
#        [5.7, 3.8, 1.7, 0.3],
#        [5.1, 3.8, 1.5, 0.3],
#        [5.4, 3.4, 1.7, 0.2],
#        [5.1, 3.7, 1.5, 0.4],
#        [4.6, 3.6, 1. , 0.2],
#        [5.1, 3.3, 1.7, 0.5],
#        [4.8, 3.4, 1.9, 0.2],
#        [5. , 3. , 1.6, 0.2],
#        [5. , 3.4, 1.6, 0.4],
#        [5.2, 3.5, 1.5, 0.2],
#        [5.2, 3.4, 1.4, 0.2],
#        [4.7, 3.2, 1.6, 0.2],
#        [4.8, 3.1, 1.6, 0.2],
#        [5.4, 3.4, 1.5, 0.4],
#        [5.2, 4.1, 1.5, 0.1],
#        [5.5, 4.2, 1.4, 0.2],
#        [4.9, 3.1, 1.5, 0.2],
#        [5. , 3.2, 1.2, 0.2],
#        [5.5, 3.5, 1.3, 0.2],
#        [4.9, 3.6, 1.4, 0.1],
#        [4.4, 3. , 1.3, 0.2],
#        [5.1, 3.4, 1.5, 0.2],
#        [5. , 3.5, 1.3, 0.3],
#        [4.5, 2.3, 1.3, 0.3],
#        [4.4, 3.2, 1.3, 0.2],
#        [5. , 3.5, 1.6, 0.6],
#        [5.1, 3.8, 1.9, 0.4],
#        [4.8, 3. , 1.4, 0.3],
#        [5.1, 3.8, 1.6, 0.2],
#        [4.6, 3.2, 1.4, 0.2],
#        [5.3, 3.7, 1.5, 0.2],
#        [5. , 3.3, 1.4, 0.2],
#        [7. , 3.2, 4.7, 1.4],
#        [6.4, 3.2, 4.5, 1.5],
#        [6.9, 3.1, 4.9, 1.5],
#        [5.5, 2.3, 4. , 1.3],
#        [6.5, 2.8, 4.6, 1.5],
#        [5.7, 2.8, 4.5, 1.3],
#        [6.3, 3.3, 4.7, 1.6],
#        [4.9, 2.4, 3.3, 1. ],
#        [6.6, 2.9, 4.6, 1.3],
#        [5.2, 2.7, 3.9, 1.4],
#        [5. , 2. , 3.5, 1. ],
#        [5.9, 3. , 4.2, 1.5],
#        [6. , 2.2, 4. , 1. ],
#        [6.1, 2.9, 4.7, 1.4],
#        [5.6, 2.9, 3.6, 1.3],
#        [6.7, 3.1, 4.4, 1.4],
#        [5.6, 3. , 4.5, 1.5],
#        [5.8, 2.7, 4.1, 1. ],
#        [6.2, 2.2, 4.5, 1.5],
#        [5.6, 2.5, 3.9, 1.1],
#        [5.9, 3.2, 4.8, 1.8],
#        [6.1, 2.8, 4. , 1.3],
#        [6.3, 2.5, 4.9, 1.5],
#        [6.1, 2.8, 4.7, 1.2],
#        [6.4, 2.9, 4.3, 1.3],
#        [6.6, 3. , 4.4, 1.4],
#        [6.8, 2.8, 4.8, 1.4],
#        [6.7, 3. , 5. , 1.7],
#        [6. , 2.9, 4.5, 1.5],
#        [5.7, 2.6, 3.5, 1. ],
#        [5.5, 2.4, 3.8, 1.1],
#        [5.5, 2.4, 3.7, 1. ],
#        [5.8, 2.7, 3.9, 1.2],
#        [6. , 2.7, 5.1, 1.6],
#        [5.4, 3. , 4.5, 1.5],
#        [6. , 3.4, 4.5, 1.6],
#        [6.7, 3.1, 4.7, 1.5],
#        [6.3, 2.3, 4.4, 1.3],
#        [5.6, 3. , 4.1, 1.3],
#        [5.5, 2.5, 4. , 1.3],
#        [5.5, 2.6, 4.4, 1.2],
#        [6.1, 3. , 4.6, 1.4],
#        [5.8, 2.6, 4. , 1.2],
#        [5. , 2.3, 3.3, 1. ],
#        [5.6, 2.7, 4.2, 1.3],
#        [5.7, 3. , 4.2, 1.2],
#        [5.7, 2.9, 4.2, 1.3],
#        [6.2, 2.9, 4.3, 1.3],
#        [5.1, 2.5, 3. , 1.1],
#        [5.7, 2.8, 4.1, 1.3],
#        [6.3, 3.3, 6. , 2.5],
#        [5.8, 2.7, 5.1, 1.9],
#        [7.1, 3. , 5.9, 2.1],
#        [6.3, 2.9, 5.6, 1.8],
#        [6.5, 3. , 5.8, 2.2],
#        [7.6, 3. , 6.6, 2.1],
#        [4.9, 2.5, 4.5, 1.7],
#        [7.3, 2.9, 6.3, 1.8],
#        [6.7, 2.5, 5.8, 1.8],
#        [7.2, 3.6, 6.1, 2.5],
#        [6.5, 3.2, 5.1, 2. ],
#        [6.4, 2.7, 5.3, 1.9],
#        [6.8, 3. , 5.5, 2.1],
#        [5.7, 2.5, 5. , 2. ],
#        [5.8, 2.8, 5.1, 2.4],
#        [6.4, 3.2, 5.3, 2.3],
#        [6.5, 3. , 5.5, 1.8],
#        [7.7, 3.8, 6.7, 2.2],
#        [7.7, 2.6, 6.9, 2.3],
#        [6. , 2.2, 5. , 1.5],
#        [6.9, 3.2, 5.7, 2.3],
#        [5.6, 2.8, 4.9, 2. ],
#        [7.7, 2.8, 6.7, 2. ],
#        [6.3, 2.7, 4.9, 1.8],
#        [6.7, 3.3, 5.7, 2.1],
#        [7.2, 3.2, 6. , 1.8],
#        [6.2, 2.8, 4.8, 1.8],
#        [6.1, 3. , 4.9, 1.8],
#        [6.4, 2.8, 5.6, 2.1],
#        [7.2, 3. , 5.8, 1.6],
#        [7.4, 2.8, 6.1, 1.9],
#        [7.9, 3.8, 6.4, 2. ],
#        [6.4, 2.8, 5.6, 2.2],
#        [6.3, 2.8, 5.1, 1.5],
#        [6.1, 2.6, 5.6, 1.4],
#        [7.7, 3. , 6.1, 2.3],
#        [6.3, 3.4, 5.6, 2.4],
#        [6.4, 3.1, 5.5, 1.8],
#        [6. , 3. , 4.8, 1.8],
#        [6.9, 3.1, 5.4, 2.1],
#        [6.7, 3.1, 5.6, 2.4],
#        [6.9, 3.1, 5.1, 2.3],
#        [5.8, 2.7, 5.1, 1.9],
#        [6.8, 3.2, 5.9, 2.3],
#        [6.7, 3.3, 5.7, 2.5],
#        [6.7, 3. , 5.2, 2.3],
#        [6.3, 2.5, 5. , 1.9],
#        [6.5, 3. , 5.2, 2. ],
#        [6.2, 3.4, 5.4, 2.3],
#        [5.9, 3. , 5.1, 1.8]])




# iris.target 는 레이블 (label, 정답) 데이터 (ndarray 객체)
iris_label = iris.target

iris_label_name = iris.target_names

print(iris_label)
print(iris_label_name);
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2]
# ['setosa' 'versicolor' 'virginica']




# 데이터 세트를 데이터프레임으로 변환
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df.head()
#   sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)	label
# 0	5.1	3.5	1.4	0.2	0
# 1	4.9	3.0	1.4	0.2	0
# 2	4.7	3.2	1.3	0.2	0
# 3	4.6	3.1	1.5	0.2	0
# 4	5.0	3.6	1.4	0.2	0




# 학습용 데이터와 테스트용 데이터를 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)   # (학습할 데이터, 정답, 떼어낼 크기, 시드값)  => 셔플 후에 일부 데이터를 뽑음   튜플로 줌
# 인공지능 모델 알고리즘 선택
dt_clf = DecisionTreeClassifier(random_state=11)

# 학습 수행
dt_clf.fit(X_train, y_train)
# DecisionTreeClassifier(random_state=11)

# 학습이 완료된 모델이 새로운 데이터를 가지고 분류를 수행
pred = dt_clf.predict(X_test)
pred
# array([2, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,
#        0, 1, 0, 0, 2, 1, 0, 1])




# 위 수행한 (예측한) 결과를 바탕으로 만든 인공지능 모델의 성능을 평가
from sklearn.metrics import accuracy_score

print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))
# 예측 정확도 : 0.9333



# ### 프로세스 정리 (지도학습 - 분류)
# * 데이터 로딩
# * 데이터 탐색
# * 데이터 세트 분리 : 학습데이터와 테스트데이터를 분리
# * 모델 학습 : 학습데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킴
# * 예측 수행 : 학습된 ML 모델을 이용하여 테스트데이터의 분류를 예측
# * 평가 : 예측 수행을 통해 예측된 결과값과 테스트데이터의 실제 값(레이블 값) 을 비교해서 학습된 모델의 성능을 평가

# # 데이터 전처리
# ## Garbage In, Garbage Out.
# ## 사이킷런의 ML 알고리즘에 적용하기 전에 데이터에 대해 밀 처리해야할 기본사항
# * 결손값. 즉, NaN, Null 값은 허용되지 않는다
# 1. Drop
# 2. 대체값 선정

# * 사이킷런의 ML 알고리즘들은 입력값으로 문자열을 허용하지 않는다.
# 1. Drop
# 2. 인코딩



# ## 데이터 인코딩
# * 레이블 인코딩 (Label Encoding)
#  : 카테고리 피처를 코드형 숫자값으로 바꾸는 방식


from sklearn.preprocessing import LabelEncoder

items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']

encoder = LabelEncoder()
encoder.fit(items)

labels = encoder.transform(items)
print('인코딩 반환값 : ', labels)
# 인코딩 반환값 :  [0 1 4 5 3 3 2 2]



print('인코딩 클래스 : ', encoder.classes_)
# 인코딩 클래스 :  ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']


# 원본 정보가 있으므로 디코딩 가능
print('디코딩 값 : ', encoder.inverse_transform([4, 5, 1, 1, 2, 3, 4, 0, 2, 2]))
# 디코딩 값 :  ['전자레인지' '컴퓨터' '냉장고' '냉장고' '믹서' '선풍기' '전자레인지' 'TV' '믹서' '믹서']



# '''
# 주의
#  : 레이블 인코딩은 레이블이 숫자로 증가하는 형태로 변환되는 특성이 있다.
#  그러나 특정 ML 알고리즘은 숫자 특성에 영향을 받음.
#  예를 들어 2는 1보다 더 큰 값이라고 인식하도록 설계되어 있는 알고리즘은 가중치가 부여되어 
#  더 중요하다고 판단할 수 있기 떄문이다. (선형회귀와 같은 알고리즘)
#  그러므로 이런 ML 알고리즘에는 레이블 인코딩을 사용하는 것을 지양.
# '''


# * 원-핫 인코딩 (One-Hot Encoding)

# : 고유값에 해당하는 컬럼만 1을 표시하고
# 나머지는 0으로 표시



from sklearn.preprocessing import OneHotEncoder

items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']

# 먼저 숫자값으로 변경 - LabelEncoder 사용
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)

labels = labels.reshape(-1, 1)

labels
# array([[0],
#        [1],
#        [4],
#        [5],
#        [3],
#        [3],
#        [2],
#        [2]])




# 원-핫 인코딩을 적용
oh_encoder = OneHotEncoder()
oh_encoder.fit(labels)
oh_labels = oh_encoder.transform(labels)
print('--- 원-핫 인코딩 데이터')
print(oh_labels.toarray())
print('--- 원-핫 인코딩 데이터 차원')
print(oh_labels.shape)
# --- 원-핫 인코딩 데이터
# [[1. 0. 0. 0. 0. 0.]
#  [0. 1. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 1. 0.]
#  [0. 0. 0. 0. 0. 1.]
#  [0. 0. 0. 1. 0. 0.]
#  [0. 0. 0. 1. 0. 0.]
#  [0. 0. 1. 0. 0. 0.]
#  [0. 0. 1. 0. 0. 0.]]
# --- 원-핫 인코딩 데이터 차원
# (8, 6)




# 판다스가 제공하는 원-핫 인코딩
df = pd.DataFrame({'item': ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})
pd.get_dummies(df)
# 	item_TV	item_냉장고	item_믹서	item_선풍기	item_전자레인지	item_컴퓨터
# 0	1	0	0	0	0	0
# 1	0	1	0	0	0	0
# 2	0	0	0	0	1	0
# 3	0	0	0	0	0	1
# 4	0	0	0	1	0	0
# 5	0	0	0	1	0	0
# 6	0	0	1	0	0	0
# 7	0	0	1	0	0	0




# # 피처스케일링과 정규화

# * 피처스케일링 : 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업

# * 피처스케일링의 대표적 작업 : 표준화(Standardization), 정규화(Normalization)

# * 표준화 : 데이터 피처 각각의 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것

# * 정규화 : 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념




from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
irisData = iris.data
irisDf = pd.DataFrame(data=irisData, columns = iris.feature_names)
irisDf.head()
# sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
# 0	5.1	3.5	1.4	0.2
# 1	4.9	3.0	1.4	0.2
# 2	4.7	3.2	1.3	0.2
# 3	4.6	3.1	1.5	0.2
# 4	5.0	3.6	1.4	0.2




# 평균값
iris_df.mean()
# sepal length (cm)    5.843333
# sepal width (cm)     3.057333
# petal length (cm)    3.758000
# petal width (cm)     1.199333
# dtype: float64



# 분산값
iris_df.var()
# sepal length (cm)    0.685694
# sepal width (cm)     0.189979
# petal length (cm)    3.116278
# petal width (cm)     0.581006
# dtype: float64




# * StandardScalar - 표준화를 쉽게 지원하기 위한 사이킷런 클래스



from sklearn.preprocessing import StandardScaler

# 객체 생성
scaler = StandardScaler()

scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

iris_scaled_df = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

print(iris_scaled_df.mean())
print('----------------------')
print(iris_scaled_df.var())
# sepal length (cm)   -1.690315e-15
# sepal width (cm)    -1.842970e-15
# petal length (cm)   -1.698641e-15
# petal width (cm)    -1.409243e-15
# dtype: float64
# ----------------------
# sepal length (cm)    1.006711
# sepal width (cm)     1.006711
# petal length (cm)    1.006711
# petal width (cm)     1.006711
# dtype: float64




# svm, 선형회귀, 로지스틱 회귀 알고리즘 등은 데이터가 가우시안 분포를 가지고 있다고
#   가정을 하고 설계되어 있는 알고리즘. 사전에 표준화 작업을 하는 것은 이런 알고리즘
#   을 통한 예측 성능 향상에 중요한 요소가 될 수 있다.



# * MinMaxScaler
# : 데이터를 0과 1 사이의 범위 값으로 변환 (음수값이 있으면 -1을 1값으로 변환)
# : 데이터의 분포가 가우시안 분포가 아닐 경우에 유용



from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler 객체 생성
# 데이터셋 변환
# 표준화 결과를 데이터프레임 형태로 만들기
# 각 피처에 대해서 평균값, 분산값 추출
# min_df



# MinMaxScaler 객체 생성
mmScaler = MinMaxScaler()

# 데이터셋 변환
mmScaler.fit(iris_df)
iris_scaled = mmScaler.transform(iris_df)

# 표준화 결과를 데이터프레임 형태로 만들기
iris_scaled_df = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

# 각 피처에 대해서 평균값, 분산값 추출  
print(iris_scaled_df.mean())
print('------------------------')
print(iris_scaled_df.var())





# # 평가 (Evaluation)
# ## 성능평가지표
# * 정확도 (Accuracy)
# * 오차행렬 (Confusion Matrix)
# * 정밀도 ()
# * 재현율 ()
# * F1스코어 ()
# * ROC AUC ()


## 1. Accuracy (정확도)

# '''
# # 정확도 : 실제 데이터와 예측 데이터가 얼마나 같은지 판단하는 지표
#           : 직관적으로 모델 예측 성능을 나타내는 지표

#                     예측 결과와 동일한 데이터 건수
#         정확도 =  -----------------------------------
#                     전체 데이터 예측 건수
                
#           : 이거 하나만으로 성능을 평가하는 것은 지양 - 데이터 왜곡이 있을 수 있음.
          
# '''




# 2. Confusion Matrix (오차 행렬)

from IPython.display import Image

from google.colab import files
files.upload()
# Image('confusion_matrix.png')

# '''
# # 오차행렬 (혼동행렬)
#  : 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고(confusion) 있는지도 함께 보여주는 지표

# - TP : 예측값을 positive 1, 실제값이 positive 1인 경우
# - TN : 예측값을 negative 0, 실제값이 negative 0인 경우
# - FP : 예측값을 positive 1, 실제값이 negative 0인 경우
# - FN : 예측값을 negative 0, 실제값이 positive 1인 경우
#                   (TP + TN)
# 정확도 = -----------------------------
#             (TP + TN + FP + FN)
# '''



## 3. 정밀도(Precision) 와 재현율(Recall)

# '''
# # 업무 특성에 따라서 특정 평가 지표가 더 중요한 경우가 있음
# - 재현율이 상대적으로 더 중요한 경우
#   재현율 = TP / (FN + TP)
#   : 실제 양성 중 정확히 양성이라고 식별된 사례의 비율
#   : 실제 Positive 양성인 데이터 예측을 Negative라고 잘못 판단했을 경우 업무상 큰 영향이 발생할 경우

#   ex) 암 환자 판별하는 모델, 금융 사기 적발 모델

# - 정밀도가 상대적으로 더 중요한 경우
#   정밀도 = TP / (FP + TP)
#   : 예측을 Positive로 한 대상 중에서 예측과 실제 값이 Positive로 일치한 데이터 비율
#   : 실제 Negative인 데이터 예측을 Positive로 잘못 판단하게 되면 업무상 큰 영향이 발생할 경우

#   ex) 스팸 메일 판별 모델

# : 가장 좋은 성능은 둘 모두 높은 수치를 얻는 것.
#   이 둘의 관계는 상호 보완적이므로 어느 한 쪽이 너무 높고, 다른 수치는 매우 낮다면
#   바람직하지 않다.

# '''



# 4. F1 Score

# '''
# # F1 Score : 정밀도와 재현율을 결합한 지표
#             : 정밀도와 재현율이 어느 한쪽으로 치우쳐지지 않은 수치를 나타낼 때
#               f1 scorre도 상대적으로 높은 수치를 보인다.
# '''



# 5. ROC Curve와 AUC

# '''
# # ROC (Receiver Operation Curve) : 수신자 판단 곡선
#   : FPR (False Positive Rate) 이 변할 때 TPR(재현율, 정밀도) 이 어떻게 변하는 지를 나타내는 곡선
# '''



from google.colab import files
files.upload()
diabetes.csv
# diabetes.csv(text/csv) - 23105 bytes, last modified: 2022. 7. 13. - 100% done
# Saving diabetes.csv to diabetes.csv
# {'diabetes.csv': b'Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome\n6,148,72,35,0,33.6,0.627,50,1\n1,85,66,29,0,26.6,0.351,31,0\n8,183,64,0,0,23.3,0.672,32,1\n1,89,66,23,94,28.1,0.167,21,0\n0,137,40,35,168,43.1,2.288,33,1\n5,116,74,0,0,25.6,0.201,30,0\n3,78,50,32,88,31,0.248,26,1\n10,115,0,0,0,35.3,0.134,29,0\n2,197,70,45,543,30.5,0.158,53,1\n8,125,96,0,0,0,0.232,54,1\n4,110,92,0,0,37.6,0.191,30,0\n10,168,74,0,0,38,0.537,34,1\n10,139,80,0,0,27.1,1.441,57,0\n1,189,60,23,846,30.1,0.398,59,1\n5,166,72,19,175,25.8,0.587,51,1\n7,100,0,0,0,30,0.484,32,1\n0,118,84,47,230,45.8,0.551,31,1\n7,107,74,0,0,29.6,0.254,31,1\n1,103,30,38,83,43.3,0.183,33,0\n1,115,70,30,96,34.6,0.529,32,1\n3,126,88,41,235,39.3,0.704,27,0\n8,99,84,0,0,35.4,0.388,50,0\n7,196,90,0,0,39.8,0.451,41,1\n9,119,80,35,0,29,0.263,29,1\n11,143,94,33,146,36.6,0.254,51,1\n10,125,70,26,115,31.1,0.205,41,1\n7,147,76,0,0,39.4,0.257,43,1\n1,97,66,15,140,23.2,0.487,22,0\n13,145,82,19,110,22.2,0.245,57,0\n5,117,92,0,0,34.1,0.337,38,0\n5,109,75,26,0,36,0.546,60,0\n3,158,76,36,245,31.6,0.851,28,1\n3,88,58,11,54,24.8,0.267,22,0\n6,92,92,0,0,19.9,0.188,28,0\n10,122,78,31,0,27.6,0.512,45,0\n4,103,60,33,192,24,0.966,33,0\n11,138,76,0,0,33.2,0.42,35,0\n9,102,76,37,0,32.9,0.665,46,1\n2,90,68,42,0,38.2,0.503,27,1\n4,111,72,47,207,37.1,1.39,56,1\n3,180,64,25,70,34,0.271,26,0\n7,133,84,0,0,40.2,0.696,37,0\n7,106,92,18,0,22.7,0.235,48,0\n9,171,110,24,240,45.4,0.721,54,1\n7,159,64,0,0,27.4,0.294,40,0\n0,180,66,39,0,42,1.893,25,1\n1,146,56,0,0,29.7,0.564,29,0\n2,71,70,27,0,28,0.586,22,0\n7,103,66,32,0,39.1,0.344,31,1\n7,105,0,0,0,0,0.305,24,0\n1,103,80,11,82,19.4,0.491,22,0\n1,101,50,15,36,24.2,0.526,26,0\n5,88,66,21,23,24.4,0.342,30,0\n8,176,90,34,300,33.7,0.467,58,1\n7,150,66,42,342,34.7,0.718,42,0\n1,73,50,10,0,23,0.248,21,0\n7,187,68,39,304,37.7,0.254,41,1\n0,100,88,60,110,46.8,0.962,31,0\n0,146,82,0,0,40.5,1.781,44,0\n0,105,64,41,142,41.5,0.173,22,0\n2,84,0,0,0,0,0.304,21,0\n8,133,72,0,0,32.9,0.27,39,1\n5,44,62,0,0,25,0.587,36,0\n2,141,58,34,128,25.4,0.699,24,0\n7,114,66,0,0,32.8,0.258,42,1\n5,99,74,27,0,29,0.203,32,0\n0,109,88,30,0,32.5,0.855,38,1\n2,109,92,0,0,42.7,0.845,54,0\n1,95,66,13,38,19.6,0.334,25,0\n4,146,85,27,100,28.9,0.189,27,0\n2,100,66,20,90,32.9,0.867,28,1\n5,139,64,35,140,28.6,0.411,26,0\n13,126,90,0,0,43.4,0.583,42,1\n4,129,86,20,270,35.1,0.231,23,0\n1,79,75,30,0,32,0.396,22,0\n1,0,48,20,0,24.7,0.14,22,0\n7,62,78,0,0,32.6,0.391,41,0\n5,95,72,33,0,37.7,0.37,27,0\n0,131,0,0,0,43.2,0.27,26,1\n2,112,66,22,0,25,0.307,24,0\n3,113,44,13,0,22.4,0.14,22,0\n2,74,0,0,0,0,0.102,22,0\n7,83,78,26,71,29.3,0.767,36,0\n0,101,65,28,0,24.6,0.237,22,0\n5,137,108,0,0,48.8,0.227,37,1\n2,110,74,29,125,32.4,0.698,27,0\n13,106,72,54,0,36.6,0.178,45,0\n2,100,68,25,71,38.5,0.324,26,0\n15,136,70,32,110,37.1,0.153,43,1\n1,107,68,19,0,26.5,0.165,24,0\n1,80,55,0,0,19.1,0.258,21,0\n4,123,80,15,176,32,0.443,34,0\n7,81,78,40,48,46.7,0.261,42,0\n4,134,72,0,0,23.8,0.277,60,1\n2,142,82,18,64,24.7,0.761,21,0\n6,144,72,27,228,33.9,0.255,40,0\n2,92,62,28,0,31.6,0.13,24,0\n1,71,48,18,76,20.4,0.323,22,0\n6,93,50,30,64,28.7,0.356,23,0\n1,122,90,51,220,49.7,0.325,31,1\n1,163,72,0,0,39,1.222,33,1\n1,151,60,0,0,26.1,0.179,22,0\n0,125,96,0,0,22.5,0.262,21,0\n1,81,72,18,40,26.6,0.283,24,0\n2,85,65,0,0,39.6,0.93,27,0\n1,126,56,29,152,28.7,0.801,21,0\n1,96,122,0,0,22.4,0.207,27,0\n4,144,58,28,140,29.5,0.287,37,0\n3,83,58,31,18,34.3,0.336,25,0\n0,95,85,25,36,37.4,0.247,24,1\n3,171,72,33,135,33.3,0.199,24,1\n8,155,62,26,495,34,0.543,46,1\n1,89,76,34,37,31.2,0.192,23,0\n4,76,62,0,0,34,0.391,25,0\n7,160,54,32,175,30.5,0.588,39,1\n4,146,92,0,0,31.2,0.539,61,1\n5,124,74,0,0,34,0.22,38,1\n5,78,48,0,0,33.7,0.654,25,0\n4,97,60,23,0,28.2,0.443,22,0\n4,99,76,15,51,23.2,0.223,21,0\n0,162,76,56,100,53.2,0.759,25,1\n6,111,64,39,0,34.2,0.26,24,0\n2,107,74,30,100,33.6,0.404,23,0\n5,132,80,0,0,26.8,0.186,69,0\n0,113,76,0,0,33.3,0.278,23,1\n1,88,30,42,99,55,0.496,26,1\n3,120,70,30,135,42.9,0.452,30,0\n1,118,58,36,94,33.3,0.261,23,0\n1,117,88,24,145,34.5,0.403,40,1\n0,105,84,0,0,27.9,0.741,62,1\n4,173,70,14,168,29.7,0.361,33,1\n9,122,56,0,0,33.3,1.114,33,1\n3,170,64,37,225,34.5,0.356,30,1\n8,84,74,31,0,38.3,0.457,39,0\n2,96,68,13,49,21.1,0.647,26,0\n2,125,60,20,140,33.8,0.088,31,0\n0,100,70,26,50,30.8,0.597,21,0\n0,93,60,25,92,28.7,0.532,22,0\n0,129,80,0,0,31.2,0.703,29,0\n5,105,72,29,325,36.9,0.159,28,0\n3,128,78,0,0,21.1,0.268,55,0\n5,106,82,30,0,39.5,0.286,38,0\n2,108,52,26,63,32.5,0.318,22,0\n10,108,66,0,0,32.4,0.272,42,1\n4,154,62,31,284,32.8,0.237,23,0\n0,102,75,23,0,0,0.572,21,0\n9,57,80,37,0,32.8,0.096,41,0\n2,106,64,35,119,30.5,1.4,34,0\n5,147,78,0,0,33.7,0.218,65,0\n2,90,70,17,0,27.3,0.085,22,0\n1,136,74,50,204,37.4,0.399,24,0\n4,114,65,0,0,21.9,0.432,37,0\n9,156,86,28,155,34.3,1.189,42,1\n1,153,82,42,485,40.6,0.687,23,0\n8,188,78,0,0,47.9,0.137,43,1\n7,152,88,44,0,50,0.337,36,1\n2,99,52,15,94,24.6,0.637,21,0\n1,109,56,21,135,25.2,0.833,23,0\n2,88,74,19,53,29,0.229,22,0\n17,163,72,41,114,40.9,0.817,47,1\n4,151,90,38,0,29.7,0.294,36,0\n7,102,74,40,105,37.2,0.204,45,0\n0,114,80,34,285,44.2,0.167,27,0\n2,100,64,23,0,29.7,0.368,21,0\n0,131,88,0,0,31.6,0.743,32,1\n6,104,74,18,156,29.9,0.722,41,1\n3,148,66,25,0,32.5,0.256,22,0\n4,120,68,0,0,29.6,0.709,34,0\n4,110,66,0,0,31.9,0.471,29,0\n3,111,90,12,78,28.4,0.495,29,0\n6,102,82,0,0,30.8,0.18,36,1\n6,134,70,23,130,35.4,0.542,29,1\n2,87,0,23,0,28.9,0.773,25,0\n1,79,60,42,48,43.5,0.678,23,0\n2,75,64,24,55,29.7,0.37,33,0\n8,179,72,42,130,32.7,0.719,36,1\n6,85,78,0,0,31.2,0.382,42,0\n0,129,110,46,130,67.1,0.319,26,1\n5,143,78,0,0,45,0.19,47,0\n5,130,82,0,0,39.1,0.956,37,1\n6,87,80,0,0,23.2,0.084,32,0\n0,119,64,18,92,34.9,0.725,23,0\n1,0,74,20,23,27.7,0.299,21,0\n5,73,60,0,0,26.8,0.268,27,0\n4,141,74,0,0,27.6,0.244,40,0\n7,194,68,28,0,35.9,0.745,41,1\n8,181,68,36,495,30.1,0.615,60,1\n1,128,98,41,58,32,1.321,33,1\n8,109,76,39,114,27.9,0.64,31,1\n5,139,80,35,160,31.6,0.361,25,1\n3,111,62,0,0,22.6,0.142,21,0\n9,123,70,44,94,33.1,0.374,40,0\n7,159,66,0,0,30.4,0.383,36,1\n11,135,0,0,0,52.3,0.578,40,1\n8,85,55,20,0,24.4,0.136,42,0\n5,158,84,41,210,39.4,0.395,29,1\n1,105,58,0,0,24.3,0.187,21,0\n3,107,62,13,48,22.9,0.678,23,1\n4,109,64,44,99,34.8,0.905,26,1\n4,148,60,27,318,30.9,0.15,29,1\n0,113,80,16,0,31,0.874,21,0\n1,138,82,0,0,40.1,0.236,28,0\n0,108,68,20,0,27.3,0.787,32,0\n2,99,70,16,44,20.4,0.235,27,0\n6,103,72,32,190,37.7,0.324,55,0\n5,111,72,28,0,23.9,0.407,27,0\n8,196,76,29,280,37.5,0.605,57,1\n5,162,104,0,0,37.7,0.151,52,1\n1,96,64,27,87,33.2,0.289,21,0\n7,184,84,33,0,35.5,0.355,41,1\n2,81,60,22,0,27.7,0.29,25,0\n0,147,85,54,0,42.8,0.375,24,0\n7,179,95,31,0,34.2,0.164,60,0\n0,140,65,26,130,42.6,0.431,24,1\n9,112,82,32,175,34.2,0.26,36,1\n12,151,70,40,271,41.8,0.742,38,1\n5,109,62,41,129,35.8,0.514,25,1\n6,125,68,30,120,30,0.464,32,0\n5,85,74,22,0,29,1.224,32,1\n5,112,66,0,0,37.8,0.261,41,1\n0,177,60,29,478,34.6,1.072,21,1\n2,158,90,0,0,31.6,0.805,66,1\n7,119,0,0,0,25.2,0.209,37,0\n7,142,60,33,190,28.8,0.687,61,0\n1,100,66,15,56,23.6,0.666,26,0\n1,87,78,27,32,34.6,0.101,22,0\n0,101,76,0,0,35.7,0.198,26,0\n3,162,52,38,0,37.2,0.652,24,1\n4,197,70,39,744,36.7,2.329,31,0\n0,117,80,31,53,45.2,0.089,24,0\n4,142,86,0,0,44,0.645,22,1\n6,134,80,37,370,46.2,0.238,46,1\n1,79,80,25,37,25.4,0.583,22,0\n4,122,68,0,0,35,0.394,29,0\n3,74,68,28,45,29.7,0.293,23,0\n4,171,72,0,0,43.6,0.479,26,1\n7,181,84,21,192,35.9,0.586,51,1\n0,179,90,27,0,44.1,0.686,23,1\n9,164,84,21,0,30.8,0.831,32,1\n0,104,76,0,0,18.4,0.582,27,0\n1,91,64,24,0,29.2,0.192,21,0\n4,91,70,32,88,33.1,0.446,22,0\n3,139,54,0,0,25.6,0.402,22,1\n6,119,50,22,176,27.1,1.318,33,1\n2,146,76,35,194,38.2,0.329,29,0\n9,184,85,15,0,30,1.213,49,1\n10,122,68,0,0,31.2,0.258,41,0\n0,165,90,33,680,52.3,0.427,23,0\n9,124,70,33,402,35.4,0.282,34,0\n1,111,86,19,0,30.1,0.143,23,0\n9,106,52,0,0,31.2,0.38,42,0\n2,129,84,0,0,28,0.284,27,0\n2,90,80,14,55,24.4,0.249,24,0\n0,86,68,32,0,35.8,0.238,25,0\n12,92,62,7,258,27.6,0.926,44,1\n1,113,64,35,0,33.6,0.543,21,1\n3,111,56,39,0,30.1,0.557,30,0\n2,114,68,22,0,28.7,0.092,25,0\n1,193,50,16,375,25.9,0.655,24,0\n11,155,76,28,150,33.3,1.353,51,1\n3,191,68,15,130,30.9,0.299,34,0\n3,141,0,0,0,30,0.761,27,1\n4,95,70,32,0,32.1,0.612,24,0\n3,142,80,15,0,32.4,0.2,63,0\n4,123,62,0,0,32,0.226,35,1\n5,96,74,18,67,33.6,0.997,43,0\n0,138,0,0,0,36.3,0.933,25,1\n2,128,64,42,0,40,1.101,24,0\n0,102,52,0,0,25.1,0.078,21,0\n2,146,0,0,0,27.5,0.24,28,1\n10,101,86,37,0,45.6,1.136,38,1\n2,108,62,32,56,25.2,0.128,21,0\n3,122,78,0,0,23,0.254,40,0\n1,71,78,50,45,33.2,0.422,21,0\n13,106,70,0,0,34.2,0.251,52,0\n2,100,70,52,57,40.5,0.677,25,0\n7,106,60,24,0,26.5,0.296,29,1\n0,104,64,23,116,27.8,0.454,23,0\n5,114,74,0,0,24.9,0.744,57,0\n2,108,62,10,278,25.3,0.881,22,0\n0,146,70,0,0,37.9,0.334,28,1\n10,129,76,28,122,35.9,0.28,39,0\n7,133,88,15,155,32.4,0.262,37,0\n7,161,86,0,0,30.4,0.165,47,1\n2,108,80,0,0,27,0.259,52,1\n7,136,74,26,135,26,0.647,51,0\n5,155,84,44,545,38.7,0.619,34,0\n1,119,86,39,220,45.6,0.808,29,1\n4,96,56,17,49,20.8,0.34,26,0\n5,108,72,43,75,36.1,0.263,33,0\n0,78,88,29,40,36.9,0.434,21,0\n0,107,62,30,74,36.6,0.757,25,1\n2,128,78,37,182,43.3,1.224,31,1\n1,128,48,45,194,40.5,0.613,24,1\n0,161,50,0,0,21.9,0.254,65,0\n6,151,62,31,120,35.5,0.692,28,0\n2,146,70,38,360,28,0.337,29,1\n0,126,84,29,215,30.7,0.52,24,0\n14,100,78,25,184,36.6,0.412,46,1\n8,112,72,0,0,23.6,0.84,58,0\n0,167,0,0,0,32.3,0.839,30,1\n2,144,58,33,135,31.6,0.422,25,1\n5,77,82,41,42,35.8,0.156,35,0\n5,115,98,0,0,52.9,0.209,28,1\n3,150,76,0,0,21,0.207,37,0\n2,120,76,37,105,39.7,0.215,29,0\n10,161,68,23,132,25.5,0.326,47,1\n0,137,68,14,148,24.8,0.143,21,0\n0,128,68,19,180,30.5,1.391,25,1\n2,124,68,28,205,32.9,0.875,30,1\n6,80,66,30,0,26.2,0.313,41,0\n0,106,70,37,148,39.4,0.605,22,0\n2,155,74,17,96,26.6,0.433,27,1\n3,113,50,10,85,29.5,0.626,25,0\n7,109,80,31,0,35.9,1.127,43,1\n2,112,68,22,94,34.1,0.315,26,0\n3,99,80,11,64,19.3,0.284,30,0\n3,182,74,0,0,30.5,0.345,29,1\n3,115,66,39,140,38.1,0.15,28,0\n6,194,78,0,0,23.5,0.129,59,1\n4,129,60,12,231,27.5,0.527,31,0\n3,112,74,30,0,31.6,0.197,25,1\n0,124,70,20,0,27.4,0.254,36,1\n13,152,90,33,29,26.8,0.731,43,1\n2,112,75,32,0,35.7,0.148,21,0\n1,157,72,21,168,25.6,0.123,24,0\n1,122,64,32,156,35.1,0.692,30,1\n10,179,70,0,0,35.1,0.2,37,0\n2,102,86,36,120,45.5,0.127,23,1\n6,105,70,32,68,30.8,0.122,37,0\n8,118,72,19,0,23.1,1.476,46,0\n2,87,58,16,52,32.7,0.166,25,0\n1,180,0,0,0,43.3,0.282,41,1\n12,106,80,0,0,23.6,0.137,44,0\n1,95,60,18,58,23.9,0.26,22,0\n0,165,76,43,255,47.9,0.259,26,0\n0,117,0,0,0,33.8,0.932,44,0\n5,115,76,0,0,31.2,0.343,44,1\n9,152,78,34,171,34.2,0.893,33,1\n7,178,84,0,0,39.9,0.331,41,1\n1,130,70,13,105,25.9,0.472,22,0\n1,95,74,21,73,25.9,0.673,36,0\n1,0,68,35,0,32,0.389,22,0\n5,122,86,0,0,34.7,0.29,33,0\n8,95,72,0,0,36.8,0.485,57,0\n8,126,88,36,108,38.5,0.349,49,0\n1,139,46,19,83,28.7,0.654,22,0\n3,116,0,0,0,23.5,0.187,23,0\n3,99,62,19,74,21.8,0.279,26,0\n5,0,80,32,0,41,0.346,37,1\n4,92,80,0,0,42.2,0.237,29,0\n4,137,84,0,0,31.2,0.252,30,0\n3,61,82,28,0,34.4,0.243,46,0\n1,90,62,12,43,27.2,0.58,24,0\n3,90,78,0,0,42.7,0.559,21,0\n9,165,88,0,0,30.4,0.302,49,1\n1,125,50,40,167,33.3,0.962,28,1\n13,129,0,30,0,39.9,0.569,44,1\n12,88,74,40,54,35.3,0.378,48,0\n1,196,76,36,249,36.5,0.875,29,1\n5,189,64,33,325,31.2,0.583,29,1\n5,158,70,0,0,29.8,0.207,63,0\n5,103,108,37,0,39.2,0.305,65,0\n4,146,78,0,0,38.5,0.52,67,1\n4,147,74,25,293,34.9,0.385,30,0\n5,99,54,28,83,34,0.499,30,0\n6,124,72,0,0,27.6,0.368,29,1\n0,101,64,17,0,21,0.252,21,0\n3,81,86,16,66,27.5,0.306,22,0\n1,133,102,28,140,32.8,0.234,45,1\n3,173,82,48,465,38.4,2.137,25,1\n0,118,64,23,89,0,1.731,21,0\n0,84,64,22,66,35.8,0.545,21,0\n2,105,58,40,94,34.9,0.225,25,0\n2,122,52,43,158,36.2,0.816,28,0\n12,140,82,43,325,39.2,0.528,58,1\n0,98,82,15,84,25.2,0.299,22,0\n1,87,60,37,75,37.2,0.509,22,0\n4,156,75,0,0,48.3,0.238,32,1\n0,93,100,39,72,43.4,1.021,35,0\n1,107,72,30,82,30.8,0.821,24,0\n0,105,68,22,0,20,0.236,22,0\n1,109,60,8,182,25.4,0.947,21,0\n1,90,62,18,59,25.1,1.268,25,0\n1,125,70,24,110,24.3,0.221,25,0\n1,119,54,13,50,22.3,0.205,24,0\n5,116,74,29,0,32.3,0.66,35,1\n8,105,100,36,0,43.3,0.239,45,1\n5,144,82,26,285,32,0.452,58,1\n3,100,68,23,81,31.6,0.949,28,0\n1,100,66,29,196,32,0.444,42,0\n5,166,76,0,0,45.7,0.34,27,1\n1,131,64,14,415,23.7,0.389,21,0\n4,116,72,12,87,22.1,0.463,37,0\n4,158,78,0,0,32.9,0.803,31,1\n2,127,58,24,275,27.7,1.6,25,0\n3,96,56,34,115,24.7,0.944,39,0\n0,131,66,40,0,34.3,0.196,22,1\n3,82,70,0,0,21.1,0.389,25,0\n3,193,70,31,0,34.9,0.241,25,1\n4,95,64,0,0,32,0.161,31,1\n6,137,61,0,0,24.2,0.151,55,0\n5,136,84,41,88,35,0.286,35,1\n9,72,78,25,0,31.6,0.28,38,0\n5,168,64,0,0,32.9,0.135,41,1\n2,123,48,32,165,42.1,0.52,26,0\n4,115,72,0,0,28.9,0.376,46,1\n0,101,62,0,0,21.9,0.336,25,0\n8,197,74,0,0,25.9,1.191,39,1\n1,172,68,49,579,42.4,0.702,28,1\n6,102,90,39,0,35.7,0.674,28,0\n1,112,72,30,176,34.4,0.528,25,0\n1,143,84,23,310,42.4,1.076,22,0\n1,143,74,22,61,26.2,0.256,21,0\n0,138,60,35,167,34.6,0.534,21,1\n3,173,84,33,474,35.7,0.258,22,1\n1,97,68,21,0,27.2,1.095,22,0\n4,144,82,32,0,38.5,0.554,37,1\n1,83,68,0,0,18.2,0.624,27,0\n3,129,64,29,115,26.4,0.219,28,1\n1,119,88,41,170,45.3,0.507,26,0\n2,94,68,18,76,26,0.561,21,0\n0,102,64,46,78,40.6,0.496,21,0\n2,115,64,22,0,30.8,0.421,21,0\n8,151,78,32,210,42.9,0.516,36,1\n4,184,78,39,277,37,0.264,31,1\n0,94,0,0,0,0,0.256,25,0\n1,181,64,30,180,34.1,0.328,38,1\n0,135,94,46,145,40.6,0.284,26,0\n1,95,82,25,180,35,0.233,43,1\n2,99,0,0,0,22.2,0.108,23,0\n3,89,74,16,85,30.4,0.551,38,0\n1,80,74,11,60,30,0.527,22,0\n2,139,75,0,0,25.6,0.167,29,0\n1,90,68,8,0,24.5,1.138,36,0\n0,141,0,0,0,42.4,0.205,29,1\n12,140,85,33,0,37.4,0.244,41,0\n5,147,75,0,0,29.9,0.434,28,0\n1,97,70,15,0,18.2,0.147,21,0\n6,107,88,0,0,36.8,0.727,31,0\n0,189,104,25,0,34.3,0.435,41,1\n2,83,66,23,50,32.2,0.497,22,0\n4,117,64,27,120,33.2,0.23,24,0\n8,108,70,0,0,30.5,0.955,33,1\n4,117,62,12,0,29.7,0.38,30,1\n0,180,78,63,14,59.4,2.42,25,1\n1,100,72,12,70,25.3,0.658,28,0\n0,95,80,45,92,36.5,0.33,26,0\n0,104,64,37,64,33.6,0.51,22,1\n0,120,74,18,63,30.5,0.285,26,0\n1,82,64,13,95,21.2,0.415,23,0\n2,134,70,0,0,28.9,0.542,23,1\n0,91,68,32,210,39.9,0.381,25,0\n2,119,0,0,0,19.6,0.832,72,0\n2,100,54,28,105,37.8,0.498,24,0\n14,175,62,30,0,33.6,0.212,38,1\n1,135,54,0,0,26.7,0.687,62,0\n5,86,68,28,71,30.2,0.364,24,0\n10,148,84,48,237,37.6,1.001,51,1\n9,134,74,33,60,25.9,0.46,81,0\n9,120,72,22,56,20.8,0.733,48,0\n1,71,62,0,0,21.8,0.416,26,0\n8,74,70,40,49,35.3,0.705,39,0\n5,88,78,30,0,27.6,0.258,37,0\n10,115,98,0,0,24,1.022,34,0\n0,124,56,13,105,21.8,0.452,21,0\n0,74,52,10,36,27.8,0.269,22,0\n0,97,64,36,100,36.8,0.6,25,0\n8,120,0,0,0,30,0.183,38,1\n6,154,78,41,140,46.1,0.571,27,0\n1,144,82,40,0,41.3,0.607,28,0\n0,137,70,38,0,33.2,0.17,22,0\n0,119,66,27,0,38.8,0.259,22,0\n7,136,90,0,0,29.9,0.21,50,0\n4,114,64,0,0,28.9,0.126,24,0\n0,137,84,27,0,27.3,0.231,59,0\n2,105,80,45,191,33.7,0.711,29,1\n7,114,76,17,110,23.8,0.466,31,0\n8,126,74,38,75,25.9,0.162,39,0\n4,132,86,31,0,28,0.419,63,0\n3,158,70,30,328,35.5,0.344,35,1\n0,123,88,37,0,35.2,0.197,29,0\n4,85,58,22,49,27.8,0.306,28,0\n0,84,82,31,125,38.2,0.233,23,0\n0,145,0,0,0,44.2,0.63,31,1\n0,135,68,42,250,42.3,0.365,24,1\n1,139,62,41,480,40.7,0.536,21,0\n0,173,78,32,265,46.5,1.159,58,0\n4,99,72,17,0,25.6,0.294,28,0\n8,194,80,0,0,26.1,0.551,67,0\n2,83,65,28,66,36.8,0.629,24,0\n2,89,90,30,0,33.5,0.292,42,0\n4,99,68,38,0,32.8,0.145,33,0\n4,125,70,18,122,28.9,1.144,45,1\n3,80,0,0,0,0,0.174,22,0\n6,166,74,0,0,26.6,0.304,66,0\n5,110,68,0,0,26,0.292,30,0\n2,81,72,15,76,30.1,0.547,25,0\n7,195,70,33,145,25.1,0.163,55,1\n6,154,74,32,193,29.3,0.839,39,0\n2,117,90,19,71,25.2,0.313,21,0\n3,84,72,32,0,37.2,0.267,28,0\n6,0,68,41,0,39,0.727,41,1\n7,94,64,25,79,33.3,0.738,41,0\n3,96,78,39,0,37.3,0.238,40,0\n10,75,82,0,0,33.3,0.263,38,0\n0,180,90,26,90,36.5,0.314,35,1\n1,130,60,23,170,28.6,0.692,21,0\n2,84,50,23,76,30.4,0.968,21,0\n8,120,78,0,0,25,0.409,64,0\n12,84,72,31,0,29.7,0.297,46,1\n0,139,62,17,210,22.1,0.207,21,0\n9,91,68,0,0,24.2,0.2,58,0\n2,91,62,0,0,27.3,0.525,22,0\n3,99,54,19,86,25.6,0.154,24,0\n3,163,70,18,105,31.6,0.268,28,1\n9,145,88,34,165,30.3,0.771,53,1\n7,125,86,0,0,37.6,0.304,51,0\n13,76,60,0,0,32.8,0.18,41,0\n6,129,90,7,326,19.6,0.582,60,0\n2,68,70,32,66,25,0.187,25,0\n3,124,80,33,130,33.2,0.305,26,0\n6,114,0,0,0,0,0.189,26,0\n9,130,70,0,0,34.2,0.652,45,1\n3,125,58,0,0,31.6,0.151,24,0\n3,87,60,18,0,21.8,0.444,21,0\n1,97,64,19,82,18.2,0.299,21,0\n3,116,74,15,105,26.3,0.107,24,0\n0,117,66,31,188,30.8,0.493,22,0\n0,111,65,0,0,24.6,0.66,31,0\n2,122,60,18,106,29.8,0.717,22,0\n0,107,76,0,0,45.3,0.686,24,0\n1,86,66,52,65,41.3,0.917,29,0\n6,91,0,0,0,29.8,0.501,31,0\n1,77,56,30,56,33.3,1.251,24,0\n4,132,0,0,0,32.9,0.302,23,1\n0,105,90,0,0,29.6,0.197,46,0\n0,57,60,0,0,21.7,0.735,67,0\n0,127,80,37,210,36.3,0.804,23,0\n3,129,92,49,155,36.4,0.968,32,1\n8,100,74,40,215,39.4,0.661,43,1\n3,128,72,25,190,32.4,0.549,27,1\n10,90,85,32,0,34.9,0.825,56,1\n4,84,90,23,56,39.5,0.159,25,0\n1,88,78,29,76,32,0.365,29,0\n8,186,90,35,225,34.5,0.423,37,1\n5,187,76,27,207,43.6,1.034,53,1\n4,131,68,21,166,33.1,0.16,28,0\n1,164,82,43,67,32.8,0.341,50,0\n4,189,110,31,0,28.5,0.68,37,0\n1,116,70,28,0,27.4,0.204,21,0\n3,84,68,30,106,31.9,0.591,25,0\n6,114,88,0,0,27.8,0.247,66,0\n1,88,62,24,44,29.9,0.422,23,0\n1,84,64,23,115,36.9,0.471,28,0\n7,124,70,33,215,25.5,0.161,37,0\n1,97,70,40,0,38.1,0.218,30,0\n8,110,76,0,0,27.8,0.237,58,0\n11,103,68,40,0,46.2,0.126,42,0\n11,85,74,0,0,30.1,0.3,35,0\n6,125,76,0,0,33.8,0.121,54,1\n0,198,66,32,274,41.3,0.502,28,1\n1,87,68,34,77,37.6,0.401,24,0\n6,99,60,19,54,26.9,0.497,32,0\n0,91,80,0,0,32.4,0.601,27,0\n2,95,54,14,88,26.1,0.748,22,0\n1,99,72,30,18,38.6,0.412,21,0\n6,92,62,32,126,32,0.085,46,0\n4,154,72,29,126,31.3,0.338,37,0\n0,121,66,30,165,34.3,0.203,33,1\n3,78,70,0,0,32.5,0.27,39,0\n2,130,96,0,0,22.6,0.268,21,0\n3,111,58,31,44,29.5,0.43,22,0\n2,98,60,17,120,34.7,0.198,22,0\n1,143,86,30,330,30.1,0.892,23,0\n1,119,44,47,63,35.5,0.28,25,0\n6,108,44,20,130,24,0.813,35,0\n2,118,80,0,0,42.9,0.693,21,1\n10,133,68,0,0,27,0.245,36,0\n2,197,70,99,0,34.7,0.575,62,1\n0,151,90,46,0,42.1,0.371,21,1\n6,109,60,27,0,25,0.206,27,0\n12,121,78,17,0,26.5,0.259,62,0\n8,100,76,0,0,38.7,0.19,42,0\n8,124,76,24,600,28.7,0.687,52,1\n1,93,56,11,0,22.5,0.417,22,0\n8,143,66,0,0,34.9,0.129,41,1\n6,103,66,0,0,24.3,0.249,29,0\n3,176,86,27,156,33.3,1.154,52,1\n0,73,0,0,0,21.1,0.342,25,0\n11,111,84,40,0,46.8,0.925,45,1\n2,112,78,50,140,39.4,0.175,24,0\n3,132,80,0,0,34.4,0.402,44,1\n2,82,52,22,115,28.5,1.699,25,0\n6,123,72,45,230,33.6,0.733,34,0\n0,188,82,14,185,32,0.682,22,1\n0,67,76,0,0,45.3,0.194,46,0\n1,89,24,19,25,27.8,0.559,21,0\n1,173,74,0,0,36.8,0.088,38,1\n1,109,38,18,120,23.1,0.407,26,0\n1,108,88,19,0,27.1,0.4,24,0\n6,96,0,0,0,23.7,0.19,28,0\n1,124,74,36,0,27.8,0.1,30,0\n7,150,78,29,126,35.2,0.692,54,1\n4,183,0,0,0,28.4,0.212,36,1\n1,124,60,32,0,35.8,0.514,21,0\n1,181,78,42,293,40,1.258,22,1\n1,92,62,25,41,19.5,0.482,25,0\n0,152,82,39,272,41.5,0.27,27,0\n1,111,62,13,182,24,0.138,23,0\n3,106,54,21,158,30.9,0.292,24,0\n3,174,58,22,194,32.9,0.593,36,1\n7,168,88,42,321,38.2,0.787,40,1\n6,105,80,28,0,32.5,0.878,26,0\n11,138,74,26,144,36.1,0.557,50,1\n3,106,72,0,0,25.8,0.207,27,0\n6,117,96,0,0,28.7,0.157,30,0\n2,68,62,13,15,20.1,0.257,23,0\n9,112,82,24,0,28.2,1.282,50,1\n0,119,0,0,0,32.4,0.141,24,1\n2,112,86,42,160,38.4,0.246,28,0\n2,92,76,20,0,24.2,1.698,28,0\n6,183,94,0,0,40.8,1.461,45,0\n0,94,70,27,115,43.5,0.347,21,0\n2,108,64,0,0,30.8,0.158,21,0\n4,90,88,47,54,37.7,0.362,29,0\n0,125,68,0,0,24.7,0.206,21,0\n0,132,78,0,0,32.4,0.393,21,0\n5,128,80,0,0,34.6,0.144,45,0\n4,94,65,22,0,24.7,0.148,21,0\n7,114,64,0,0,27.4,0.732,34,1\n0,102,78,40,90,34.5,0.238,24,0\n2,111,60,0,0,26.2,0.343,23,0\n1,128,82,17,183,27.5,0.115,22,0\n10,92,62,0,0,25.9,0.167,31,0\n13,104,72,0,0,31.2,0.465,38,1\n5,104,74,0,0,28.8,0.153,48,0\n2,94,76,18,66,31.6,0.649,23,0\n7,97,76,32,91,40.9,0.871,32,1\n1,100,74,12,46,19.5,0.149,28,0\n0,102,86,17,105,29.3,0.695,27,0\n4,128,70,0,0,34.3,0.303,24,0\n6,147,80,0,0,29.5,0.178,50,1\n4,90,0,0,0,28,0.61,31,0\n3,103,72,30,152,27.6,0.73,27,0\n2,157,74,35,440,39.4,0.134,30,0\n1,167,74,17,144,23.4,0.447,33,1\n0,179,50,36,159,37.8,0.455,22,1\n11,136,84,35,130,28.3,0.26,42,1\n0,107,60,25,0,26.4,0.133,23,0\n1,91,54,25,100,25.2,0.234,23,0\n1,117,60,23,106,33.8,0.466,27,0\n5,123,74,40,77,34.1,0.269,28,0\n2,120,54,0,0,26.8,0.455,27,0\n1,106,70,28,135,34.2,0.142,22,0\n2,155,52,27,540,38.7,0.24,25,1\n2,101,58,35,90,21.8,0.155,22,0\n1,120,80,48,200,38.9,1.162,41,0\n11,127,106,0,0,39,0.19,51,0\n3,80,82,31,70,34.2,1.292,27,1\n10,162,84,0,0,27.7,0.182,54,0\n1,199,76,43,0,42.9,1.394,22,1\n8,167,106,46,231,37.6,0.165,43,1\n9,145,80,46,130,37.9,0.637,40,1\n6,115,60,39,0,33.7,0.245,40,1\n1,112,80,45,132,34.8,0.217,24,0\n4,145,82,18,0,32.5,0.235,70,1\n10,111,70,27,0,27.5,0.141,40,1\n6,98,58,33,190,34,0.43,43,0\n9,154,78,30,100,30.9,0.164,45,0\n6,165,68,26,168,33.6,0.631,49,0\n1,99,58,10,0,25.4,0.551,21,0\n10,68,106,23,49,35.5,0.285,47,0\n3,123,100,35,240,57.3,0.88,22,0\n8,91,82,0,0,35.6,0.587,68,0\n6,195,70,0,0,30.9,0.328,31,1\n9,156,86,0,0,24.8,0.23,53,1\n0,93,60,0,0,35.3,0.263,25,0\n3,121,52,0,0,36,0.127,25,1\n2,101,58,17,265,24.2,0.614,23,0\n2,56,56,28,45,24.2,0.332,22,0\n0,162,76,36,0,49.6,0.364,26,1\n0,95,64,39,105,44.6,0.366,22,0\n4,125,80,0,0,32.3,0.536,27,1\n5,136,82,0,0,0,0.64,69,0\n2,129,74,26,205,33.2,0.591,25,0\n3,130,64,0,0,23.1,0.314,22,0\n1,107,50,19,0,28.3,0.181,29,0\n1,140,74,26,180,24.1,0.828,23,0\n1,144,82,46,180,46.1,0.335,46,1\n8,107,80,0,0,24.6,0.856,34,0\n13,158,114,0,0,42.3,0.257,44,1\n2,121,70,32,95,39.1,0.886,23,0\n7,129,68,49,125,38.5,0.439,43,1\n2,90,60,0,0,23.5,0.191,25,0\n7,142,90,24,480,30.4,0.128,43,1\n3,169,74,19,125,29.9,0.268,31,1\n0,99,0,0,0,25,0.253,22,0\n4,127,88,11,155,34.5,0.598,28,0\n4,118,70,0,0,44.5,0.904,26,0\n2,122,76,27,200,35.9,0.483,26,0\n6,125,78,31,0,27.6,0.565,49,1\n1,168,88,29,0,35,0.905,52,1\n2,129,0,0,0,38.5,0.304,41,0\n4,110,76,20,100,28.4,0.118,27,0\n6,80,80,36,0,39.8,0.177,28,0\n10,115,0,0,0,0,0.261,30,1\n2,127,46,21,335,34.4,0.176,22,0\n9,164,78,0,0,32.8,0.148,45,1\n2,93,64,32,160,38,0.674,23,1\n3,158,64,13,387,31.2,0.295,24,0\n5,126,78,27,22,29.6,0.439,40,0\n10,129,62,36,0,41.2,0.441,38,1\n0,134,58,20,291,26.4,0.352,21,0\n3,102,74,0,0,29.5,0.121,32,0\n7,187,50,33,392,33.9,0.826,34,1\n3,173,78,39,185,33.8,0.97,31,1\n10,94,72,18,0,23.1,0.595,56,0\n1,108,60,46,178,35.5,0.415,24,0\n5,97,76,27,0,35.6,0.378,52,1\n4,83,86,19,0,29.3,0.317,34,0\n1,114,66,36,200,38.1,0.289,21,0\n1,149,68,29,127,29.3,0.349,42,1\n5,117,86,30,105,39.1,0.251,42,0\n1,111,94,0,0,32.8,0.265,45,0\n4,112,78,40,0,39.4,0.236,38,0\n1,116,78,29,180,36.1,0.496,25,0\n0,141,84,26,0,32.4,0.433,22,0\n2,175,88,0,0,22.9,0.326,22,0\n2,92,52,0,0,30.1,0.141,22,0\n3,130,78,23,79,28.4,0.323,34,1\n8,120,86,0,0,28.4,0.259,22,1\n2,174,88,37,120,44.5,0.646,24,1\n2,106,56,27,165,29,0.426,22,0\n2,105,75,0,0,23.3,0.56,53,0\n4,95,60,32,0,35.4,0.284,28,0\n0,126,86,27,120,27.4,0.515,21,0\n8,65,72,23,0,32,0.6,42,0\n2,99,60,17,160,36.6,0.453,21,0\n1,102,74,0,0,39.5,0.293,42,1\n11,120,80,37,150,42.3,0.785,48,1\n3,102,44,20,94,30.8,0.4,26,0\n1,109,58,18,116,28.5,0.219,22,0\n9,140,94,0,0,32.7,0.734,45,1\n13,153,88,37,140,40.6,1.174,39,0\n12,100,84,33,105,30,0.488,46,0\n1,147,94,41,0,49.3,0.358,27,1\n1,81,74,41,57,46.3,1.096,32,0\n3,187,70,22,200,36.4,0.408,36,1\n6,162,62,0,0,24.3,0.178,50,1\n4,136,70,0,0,31.2,1.182,22,1\n1,121,78,39,74,39,0.261,28,0\n3,108,62,24,0,26,0.223,25,0\n0,181,88,44,510,43.3,0.222,26,1\n8,154,78,32,0,32.4,0.443,45,1\n1,128,88,39,110,36.5,1.057,37,1\n7,137,90,41,0,32,0.391,39,0\n0,123,72,0,0,36.3,0.258,52,1\n1,106,76,0,0,37.5,0.197,26,0\n6,190,92,0,0,35.5,0.278,66,1\n2,88,58,26,16,28.4,0.766,22,0\n9,170,74,31,0,44,0.403,43,1\n9,89,62,0,0,22.5,0.142,33,0\n10,101,76,48,180,32.9,0.171,63,0\n2,122,70,27,0,36.8,0.34,27,0\n5,121,72,23,112,26.2,0.245,30,0\n1,126,60,0,0,30.1,0.349,47,1\n1,93,70,31,0,30.4,0.315,23,0'}




## 피마 지역 인디언 당뇨병 - 예측

# '''
# Pregnancies, (임신 횟수)
# Glucose,  (포도당 부하 검사 수치)
# BloodPressure,  (혈압)
# SkinThickness,  (팔 삼두근 뒷쪽의 피하지방 측정값)
# Insulin,  (혈청 인슐린)
# BMI,  (체질량 지수)
# DiabetesPedigreeFunction, (당뇨 내력 가중치 값)
# Age,  (나이)
# Outcome (클래스 결정 값(0 또는 1))
# '''


# 필요한 모듈
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression


# 데이터 로딩
diabetes_data = pd.read_csv('diabetes.csv')
diabetes_data.head()
#   Pregnancies	Glucose	BloodPressure	SkinThickness	Insulin	BMI	DiabetesPedigreeFunction	Age	Outcome
# 0	6	148	72	35	0	33.6	0.627	50	1
# 1	1	85	66	29	0	26.6	0.351	31	0
# 2	8	183	64	0	0	23.3	0.672	32	1
# 3	1	89	66	23	94	28.1	0.167	21	0
# 4	0	137	40	35	168	43.1	2.288	33	1



# outcome을 label로 처리 (보통 positive : 1, negative : 0)
diabetes_data.Outcome.value_counts()
# 0    500
# 1    268
# Name: Outcome, dtype: int64



# 데이터 탐색
diabetes_data.info()
# <class 'pandas.core.frame.DataFrame'>
# RangeIndex: 768 entries, 0 to 767
# Data columns (total 9 columns):
#  #   Column                    Non-Null Count  Dtype  
# ---  ------                    --------------  -----  
#  0   Pregnancies               768 non-null    int64  
#  1   Glucose                   768 non-null    int64  
#  2   BloodPressure             768 non-null    int64  
#  3   SkinThickness             768 non-null    int64  
#  4   Insulin                   768 non-null    int64  
#  5   BMI                       768 non-null    float64
#  6   DiabetesPedigreeFunction  768 non-null    float64
#  7   Age                       768 non-null    int64  
#  8   Outcome                   768 non-null    int64  
# dtypes: float64(2), int64(7)
# memory usage: 54.1 KB



# 평가 함수
def get_clf_eval(y_test, pred=None, pred_proba=None) :
  confusion = confusion_matrix(y_test, pred)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  roc_acu = roc_auc_score(y_test, pred_proba)

  # 오차행렬
  print('confusion matrix')
  print(confusion)
  print('정확도 : {0:.4f}, 정밀도 : {1:.4f}, 재현율 : {2:.4f}, \
  f1 score : {3:.4f} AUC : {4:.4f}'.format(accuracy, precision,recall,f1, roc_acu ))




# 학습데이터, 테스트데이터 준비, 분리

features = diabetes_data.iloc[:, :-1]
label = diabetes_data.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=156, stratify=label)

lr_clf = LogisticRegression(max_iter=1000)
lr_clf.fit(X_train, y_train)

pred = lr_clf.predict(X_test)

pred_proba = lr_clf.predict_proba(X_test)[:, -1]

get_clf_eval(y_test, pred, pred_proba)
# confusion matrix
# [[90 10]
#  [21 33]]
# 정확도 : 0.7987, 정밀도 : 0.7674, 재현율 : 0.6111,   f1 score : 0.6804 AUC : 0.8072